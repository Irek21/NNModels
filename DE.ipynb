{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjYxYvoHE2Efdr1UDLX8BR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irek21/nn_models/blob/master/DE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOeA4yZluhmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf1SGy0tumwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 3072\n",
        "hidden_size = 400\n",
        "neck_size = 128\n",
        "num_classes = 784\n",
        "num_epochs = 10\n",
        "batch_size = 100\n",
        "iterations = 10\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdExVXuDzjjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1857c9ad-dcea-4637-d616-e423ae11bb95"
      },
      "source": [
        "# trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "trans = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = dsets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=trans,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_dataset = dsets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=trans\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELWuxTCQ3gqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aaKSGwx3qrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvClassyfier(nn.Module): \n",
        "    def __init__(self): \n",
        "        super(ConvClassyfier, self).__init__() \n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2), \n",
        "                                    nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) \n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2), \n",
        "                                    nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2), \n",
        "                                    nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2), \n",
        "                                    nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer5 = nn.Sequential(nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=1), \n",
        "                                    nn.ReLU(), nn.MaxPool2d(kernel_size=2))\n",
        "        # self.layer6 = nn.Sequential(nn.Conv2d(512, kernel_size=))\n",
        "\n",
        "        self.drop_out = nn.Dropout() \n",
        "        self.fc1 = nn.Linear(512, 1024) \n",
        "        self.fc2 = nn.Linear(1024, 768)\n",
        "        self.fc3 = nn.Linear(768, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = x.reshape(x.size(0), -1)  \n",
        "        out = self.layer1(x) \n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out) \n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1) \n",
        "        out = self.drop_out(out) \n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out)) \n",
        "        out = F.log_softmax(self.fc3(out)) \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vkLivB2CEMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c4bbe33-0810-4d20-ac67-7da3c3f3fdfd"
      },
      "source": [
        "inds = torch.where(batch[1] != 9)\n",
        "images, labels = batch[0][inds], batch[1][inds]\n",
        "labels"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BDd0fMN4uIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "5df17f33-c9aa-4358-d327-eccb520942c6"
      },
      "source": [
        "%%time\n",
        "model = ConvClassyfier().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "total_step = len(train_loader)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   for i, batch in enumerate(train_loader):\n",
        "     indices = torch.where(batch[1] != 9)\n",
        "     images, labels = batch[0][indices], batch[1][indices]\n",
        "     images = images.to(device)\n",
        "     labels = labels.to(device)\n",
        "\n",
        "     outputs = model(images)\n",
        "     loss = criterion(outputs, labels)\n",
        "     loss_list.append(loss.item())\n",
        "\n",
        "     optimizer.zero_grad()\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     if i % batch_size == 0:\n",
        "         print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n",
        "             .format(epoch + 1, num_epochs, i, total_step, loss))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [0/500], Loss: 2.3001182079315186\n",
            "Epoch [1/10], Step [100/500], Loss: 1.9738048315048218\n",
            "Epoch [1/10], Step [200/500], Loss: 1.6955684423446655\n",
            "Epoch [1/10], Step [300/500], Loss: 1.6077343225479126\n",
            "Epoch [1/10], Step [400/500], Loss: 1.265928030014038\n",
            "Epoch [2/10], Step [0/500], Loss: 1.2663938999176025\n",
            "Epoch [2/10], Step [100/500], Loss: 1.122253656387329\n",
            "Epoch [2/10], Step [200/500], Loss: 1.1345077753067017\n",
            "Epoch [2/10], Step [300/500], Loss: 1.1319414377212524\n",
            "Epoch [2/10], Step [400/500], Loss: 1.0036706924438477\n",
            "Epoch [3/10], Step [0/500], Loss: 0.7905498147010803\n",
            "Epoch [3/10], Step [100/500], Loss: 0.8822279572486877\n",
            "Epoch [3/10], Step [200/500], Loss: 0.9122844338417053\n",
            "Epoch [3/10], Step [300/500], Loss: 0.7652236223220825\n",
            "Epoch [3/10], Step [400/500], Loss: 0.8392135500907898\n",
            "Epoch [4/10], Step [0/500], Loss: 0.8010050058364868\n",
            "Epoch [4/10], Step [100/500], Loss: 0.7716930508613586\n",
            "Epoch [4/10], Step [200/500], Loss: 0.6454837918281555\n",
            "Epoch [4/10], Step [300/500], Loss: 0.6455280184745789\n",
            "Epoch [4/10], Step [400/500], Loss: 0.7444843649864197\n",
            "Epoch [5/10], Step [0/500], Loss: 0.4890223443508148\n",
            "Epoch [5/10], Step [100/500], Loss: 0.6310271620750427\n",
            "Epoch [5/10], Step [200/500], Loss: 0.6972454786300659\n",
            "Epoch [5/10], Step [300/500], Loss: 0.4840843975543976\n",
            "Epoch [5/10], Step [400/500], Loss: 0.5282198190689087\n",
            "Epoch [6/10], Step [0/500], Loss: 0.3729417622089386\n",
            "Epoch [6/10], Step [100/500], Loss: 0.6457746028900146\n",
            "Epoch [6/10], Step [200/500], Loss: 0.6253777146339417\n",
            "Epoch [6/10], Step [300/500], Loss: 0.5258725881576538\n",
            "Epoch [6/10], Step [400/500], Loss: 0.4805176258087158\n",
            "Epoch [7/10], Step [0/500], Loss: 0.3586907982826233\n",
            "Epoch [7/10], Step [100/500], Loss: 0.5027297735214233\n",
            "Epoch [7/10], Step [200/500], Loss: 0.4300554692745209\n",
            "Epoch [7/10], Step [300/500], Loss: 0.7204980254173279\n",
            "Epoch [7/10], Step [400/500], Loss: 0.3393654227256775\n",
            "Epoch [8/10], Step [0/500], Loss: 0.20486821234226227\n",
            "Epoch [8/10], Step [100/500], Loss: 0.21163548529148102\n",
            "Epoch [8/10], Step [200/500], Loss: 0.48512664437294006\n",
            "Epoch [8/10], Step [300/500], Loss: 0.4492039978504181\n",
            "Epoch [8/10], Step [400/500], Loss: 0.3736892342567444\n",
            "Epoch [9/10], Step [0/500], Loss: 0.3102046847343445\n",
            "Epoch [9/10], Step [100/500], Loss: 0.2713310122489929\n",
            "Epoch [9/10], Step [200/500], Loss: 0.33494463562965393\n",
            "Epoch [9/10], Step [300/500], Loss: 0.33974993228912354\n",
            "Epoch [9/10], Step [400/500], Loss: 0.40972384810447693\n",
            "Epoch [10/10], Step [0/500], Loss: 0.1705590933561325\n",
            "Epoch [10/10], Step [100/500], Loss: 0.3147216737270355\n",
            "Epoch [10/10], Step [200/500], Loss: 0.16742701828479767\n",
            "Epoch [10/10], Step [300/500], Loss: 0.3519710600376129\n",
            "Epoch [10/10], Step [400/500], Loss: 0.2063249945640564\n",
            "CPU times: user 1min 51s, sys: 7.1 s, total: 1min 58s\n",
            "Wall time: 2min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eWl5ZF4TgiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle(tensor):\n",
        "  indices = torch.randperm(tensor.shape[0])\n",
        "  return tensor[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wDp28OrSlMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concat(tensor1, tensor2):\n",
        "  new_tensor = torch.cat((tensor1, tensor2), dim=0)\n",
        "  new_tensor = shuffle(new_tensor)\n",
        "  return new_tensor.view(-1, 6, 32, 32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13ZOV2AXct7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conseq_erase(tensor, oddness=0):\n",
        "  indices = [2 * i + oddness for i in range(start, tensor.shape[0] // 2)] # если хочешь использовать, необходимо дофиксить правую границу\n",
        "  return tensor[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKleZXbaKQf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvDE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvDE, self).__init__()\n",
        "        # Encoder\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(6, 32, kernel_size=5, stride=1, padding=2), \n",
        "                                    nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2), \n",
        "                                    nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout() \n",
        "        self.fc2 = nn.Linear(8 * 8 * 64 + input_size, neck_size)\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(neck_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, input_size)\n",
        "    \n",
        "    def encode(self, x, y):\n",
        "        out = self.layer1(torch.cat((x, y), dim=1))\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1) \n",
        "        out = self.drop_out(out)\n",
        "        out = F.relu(self.fc2(torch.cat((out.view(-1, 8 * 8 * 64), y.view(-1, input_size)), dim=1)))\n",
        "        return out\n",
        "    \n",
        "    def decode(self, x):\n",
        "        out = F.relu(self.fc3(x))\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        out = self.encode(x, y)\n",
        "        out = self.decode(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViTy5hrlLyfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "60536331-d84a-4b94-dce0-92765587e7b0"
      },
      "source": [
        "%%time\n",
        "deltaencoder = ConvDE().cuda()\n",
        "optimizer = torch.optim.Adam(deltaencoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "total_step = len(train_loader)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   for i, batch in enumerate(train_loader):\n",
        "     indices = torch.where(batch[1] == 9) # вообще-то 1 так-то\n",
        "     real_batch_size = len(indices)\n",
        "     oddness = real_batch_size % 2\n",
        "     images = batch[0][indices[:real_batch_size // 2]].to(device) # .view(-1, 6, 32, 32)\n",
        "     images_prev = batch[0][indices[real_batch_size // 2: real_batch_size - oddness]].to(device)\n",
        "\n",
        "     out_images = deltaencoder(images, images_prev)\n",
        "     loss = criterion(out_images, images.view(-1, input_size))\n",
        "     loss_list.append(loss.item())\n",
        "\n",
        "     optimizer.zero_grad()\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     if i % batch_size == 0:\n",
        "         print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n",
        "             .format(epoch + 1, num_epochs, i, total_step, loss))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [0/500], Loss: 0.2651723623275757\n",
            "Epoch [1/10], Step [100/500], Loss: 0.05930454283952713\n",
            "Epoch [1/10], Step [200/500], Loss: 0.042376257479190826\n",
            "Epoch [1/10], Step [300/500], Loss: 0.03582959994673729\n",
            "Epoch [1/10], Step [400/500], Loss: 0.033575572073459625\n",
            "Epoch [2/10], Step [0/500], Loss: 0.027594538405537605\n",
            "Epoch [2/10], Step [100/500], Loss: 0.025781365111470222\n",
            "Epoch [2/10], Step [200/500], Loss: 0.026697998866438866\n",
            "Epoch [2/10], Step [300/500], Loss: 0.023722229525446892\n",
            "Epoch [2/10], Step [400/500], Loss: 0.022613627836108208\n",
            "Epoch [3/10], Step [0/500], Loss: 0.022327905520796776\n",
            "Epoch [3/10], Step [100/500], Loss: 0.02812112309038639\n",
            "Epoch [3/10], Step [200/500], Loss: 0.022324975579977036\n",
            "Epoch [3/10], Step [300/500], Loss: 0.01885760761797428\n",
            "Epoch [3/10], Step [400/500], Loss: 0.018979400396347046\n",
            "Epoch [4/10], Step [0/500], Loss: 0.020746586844325066\n",
            "Epoch [4/10], Step [100/500], Loss: 0.018360694870352745\n",
            "Epoch [4/10], Step [200/500], Loss: 0.01647030934691429\n",
            "Epoch [4/10], Step [300/500], Loss: 0.01930890418589115\n",
            "Epoch [4/10], Step [400/500], Loss: 0.016459526494145393\n",
            "Epoch [5/10], Step [0/500], Loss: 0.016506729647517204\n",
            "Epoch [5/10], Step [100/500], Loss: 0.021043892949819565\n",
            "Epoch [5/10], Step [200/500], Loss: 0.016734654083848\n",
            "Epoch [5/10], Step [300/500], Loss: 0.017564130946993828\n",
            "Epoch [5/10], Step [400/500], Loss: 0.015517022460699081\n",
            "Epoch [6/10], Step [0/500], Loss: 0.01529023889452219\n",
            "Epoch [6/10], Step [100/500], Loss: 0.014807650819420815\n",
            "Epoch [6/10], Step [200/500], Loss: 0.014511903747916222\n",
            "Epoch [6/10], Step [300/500], Loss: 0.016007862985134125\n",
            "Epoch [6/10], Step [400/500], Loss: 0.015340404585003853\n",
            "Epoch [7/10], Step [0/500], Loss: 0.015452013351023197\n",
            "Epoch [7/10], Step [100/500], Loss: 0.01555581297725439\n",
            "Epoch [7/10], Step [200/500], Loss: 0.013926021754741669\n",
            "Epoch [7/10], Step [300/500], Loss: 0.013957250863313675\n",
            "Epoch [7/10], Step [400/500], Loss: 0.015623636543750763\n",
            "Epoch [8/10], Step [0/500], Loss: 0.015165885910391808\n",
            "Epoch [8/10], Step [100/500], Loss: 0.016064751893281937\n",
            "Epoch [8/10], Step [200/500], Loss: 0.014832624234259129\n",
            "Epoch [8/10], Step [300/500], Loss: 0.015569186769425869\n",
            "Epoch [8/10], Step [400/500], Loss: 0.015237686224281788\n",
            "Epoch [9/10], Step [0/500], Loss: 0.015547085553407669\n",
            "Epoch [9/10], Step [100/500], Loss: 0.013954747468233109\n",
            "Epoch [9/10], Step [200/500], Loss: 0.014813100919127464\n",
            "Epoch [9/10], Step [300/500], Loss: 0.013171461410820484\n",
            "Epoch [9/10], Step [400/500], Loss: 0.015204728581011295\n",
            "Epoch [10/10], Step [0/500], Loss: 0.013240612111985683\n",
            "Epoch [10/10], Step [100/500], Loss: 0.0175295602530241\n",
            "Epoch [10/10], Step [200/500], Loss: 0.014592285268008709\n",
            "Epoch [10/10], Step [300/500], Loss: 0.013879845850169659\n",
            "Epoch [10/10], Step [400/500], Loss: 0.012700844556093216\n",
            "CPU times: user 1min 45s, sys: 5.17 s, total: 1min 50s\n",
            "Wall time: 1min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "28ee7c15-c0f2-47be-fc3a-e5e004a5e561",
        "id": "Yq8VesGj7HKh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        }
      },
      "source": [
        "%%time\n",
        "deltaencoder = ConvDE().cuda()\n",
        "optimizer = torch.optim.Adam(deltaencoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "total_step = len(train_loader)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   for i, batch in enumerate(train_loader):\n",
        "     indices = torch.where(batch[1] == 1) # вообще-то 1 так-то\n",
        "     real_batch_size = len(indices)\n",
        "     oddness = real_batch_size % 2\n",
        "     images = batch[0][indices[:real_batch_size // 2]].to(device) # .view(-1, 6, 32, 32)\n",
        "     images_prev = batch[0][indices[real_batch_size // 2: real_batch_size - oddness]].to(device)\n",
        "\n",
        "     out_images = deltaencoder(images, images_prev)\n",
        "     loss = criterion(out_images, images.view(-1, input_size))\n",
        "     loss_list.append(loss.item())\n",
        "\n",
        "     optimizer.zero_grad()\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     if i % batch_size == 0:\n",
        "         print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n",
        "             .format(epoch + 1, num_epochs, i, total_step, loss))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [0/500], Loss: 0.2569674253463745\n",
            "Epoch [1/10], Step [100/500], Loss: 0.06958465278148651\n",
            "Epoch [1/10], Step [200/500], Loss: 0.0507856048643589\n",
            "Epoch [1/10], Step [300/500], Loss: 0.04329947009682655\n",
            "Epoch [1/10], Step [400/500], Loss: 0.04145628213882446\n",
            "Epoch [2/10], Step [0/500], Loss: 0.04017101228237152\n",
            "Epoch [2/10], Step [100/500], Loss: 0.04276104271411896\n",
            "Epoch [2/10], Step [200/500], Loss: 0.03700626268982887\n",
            "Epoch [2/10], Step [300/500], Loss: 0.04134209826588631\n",
            "Epoch [2/10], Step [400/500], Loss: 0.03507407754659653\n",
            "Epoch [3/10], Step [0/500], Loss: 0.04122506454586983\n",
            "Epoch [3/10], Step [100/500], Loss: 0.03739850968122482\n",
            "Epoch [3/10], Step [200/500], Loss: 0.04058579355478287\n",
            "Epoch [3/10], Step [300/500], Loss: 0.03640119731426239\n",
            "Epoch [3/10], Step [400/500], Loss: 0.038493622094392776\n",
            "Epoch [4/10], Step [0/500], Loss: 0.03582102060317993\n",
            "Epoch [4/10], Step [100/500], Loss: 0.04350222647190094\n",
            "Epoch [4/10], Step [200/500], Loss: 0.038233306258916855\n",
            "Epoch [4/10], Step [300/500], Loss: 0.03640264645218849\n",
            "Epoch [4/10], Step [400/500], Loss: 0.03929074853658676\n",
            "Epoch [5/10], Step [0/500], Loss: 0.03978830948472023\n",
            "Epoch [5/10], Step [100/500], Loss: 0.037635136395692825\n",
            "Epoch [5/10], Step [200/500], Loss: 0.03329699486494064\n",
            "Epoch [5/10], Step [300/500], Loss: 0.036225076764822006\n",
            "Epoch [5/10], Step [400/500], Loss: 0.03617336228489876\n",
            "Epoch [6/10], Step [0/500], Loss: 0.040420737117528915\n",
            "Epoch [6/10], Step [100/500], Loss: 0.03893951699137688\n",
            "Epoch [6/10], Step [200/500], Loss: 0.03999900817871094\n",
            "Epoch [6/10], Step [300/500], Loss: 0.039717379957437515\n",
            "Epoch [6/10], Step [400/500], Loss: 0.034886520355939865\n",
            "Epoch [7/10], Step [0/500], Loss: 0.037039902061223984\n",
            "Epoch [7/10], Step [100/500], Loss: 0.041172824800014496\n",
            "Epoch [7/10], Step [200/500], Loss: 0.04357685148715973\n",
            "Epoch [7/10], Step [300/500], Loss: 0.04003900662064552\n",
            "Epoch [7/10], Step [400/500], Loss: 0.03878437727689743\n",
            "Epoch [8/10], Step [0/500], Loss: 0.039755403995513916\n",
            "Epoch [8/10], Step [100/500], Loss: 0.038428887724876404\n",
            "Epoch [8/10], Step [200/500], Loss: 0.04412878677248955\n",
            "Epoch [8/10], Step [300/500], Loss: 0.03760160505771637\n",
            "Epoch [8/10], Step [400/500], Loss: 0.03788701072335243\n",
            "Epoch [9/10], Step [0/500], Loss: 0.03506265580654144\n",
            "Epoch [9/10], Step [100/500], Loss: 0.03877798840403557\n",
            "Epoch [9/10], Step [200/500], Loss: 0.0402156263589859\n",
            "Epoch [9/10], Step [300/500], Loss: 0.1050494983792305\n",
            "Epoch [9/10], Step [400/500], Loss: 0.044764578342437744\n",
            "Epoch [10/10], Step [0/500], Loss: 0.04292091354727745\n",
            "Epoch [10/10], Step [100/500], Loss: 0.0398935042321682\n",
            "Epoch [10/10], Step [200/500], Loss: 0.03664165735244751\n",
            "Epoch [10/10], Step [300/500], Loss: 0.035063426941633224\n",
            "Epoch [10/10], Step [400/500], Loss: 0.03582378849387169\n",
            "CPU times: user 1min 39s, sys: 4.68 s, total: 1min 44s\n",
            "Wall time: 1min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyC4lJ1tN0Ir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d992a40f-f79a-4611-f858-c4d3cda65dcb"
      },
      "source": [
        "few_shot = torch.tensor([], device=device)\n",
        "for i, batch in enumerate(train_loader):\n",
        "  indices = torch.where(batch[1] == 9)\n",
        "  images = batch[0][indices].to(device)\n",
        "  few_shot = torch.cat((few_shot, images))\n",
        "few_shot = few_shot[torch.round(torch.rand(10) * 5000).long()]\n",
        "few_shot.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6b46c096c164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mfew_shot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfew_shot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfew_shot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfew_shot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfew_shot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.43 GiB total capacity; 3.88 GiB already allocated; 2.94 MiB free; 6.91 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGYumPLC3z1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "137ef205-5649-4b95-f8f9-bdaf3d8411f0"
      },
      "source": [
        "new_dset = torch.tensor([], device=device)\n",
        "for i in range(10):\n",
        "  for j, batch in enumerate(train_loader):\n",
        "    indices = torch.where(batch[1] == 1)\n",
        "    images = batch[0][indices].to(device)\n",
        "    batch_shot = few_shot[i].expand(indices[0].shape[0], 3, 32, 32)\n",
        "    samples = deltaencoder(batch_shot, images)\n",
        "    new_dset = torch.cat((new_dset, samples.view(-1, 3, 32, 32)))\n",
        "new_dset.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cc5984b92127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_shot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfew_shot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeltaencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnew_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnew_dset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-4e443e1fb0e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-4e443e1fb0e9>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    140\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 488\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m max_pool2d = boolean_dispatch(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.43 GiB total capacity; 3.88 GiB already allocated; 2.94 MiB free; 6.91 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emG_-Q_hL_s8",
        "colab_type": "code",
        "outputId": "40c13286-e218-40f3-e231-0856ecca5b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "new_dset_norm = torch.tensor([], device=device) \n",
        "for i in range(5000):\n",
        "  sample = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)).__call__(new_dset[i])\n",
        "  new_dset_norm = torch.cat((new_dset_norm, sample.view(1, 3, 32, 32)))\n",
        "\n",
        "new_dset_norm = torch.tensor(new_dset_norm, requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzxisSh4HZ-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6d311f6-3e53-41d2-9e91-2d8edd7bec1d"
      },
      "source": [
        "new_dset_labels = torch.ones(5000, dtype=int, device=device) * 9\n",
        "new_dset_labels"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 9, 9,  ..., 9, 9, 9], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVGrYEer9NfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_generator(X, y, batch_size=1):\n",
        "    X_batch = torch.zeros(batch_size, X.shape[1], X.shape[2], X.shape[3])\n",
        "    y_batch = torch.zeros(batch_size)\n",
        "  sample = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)).__call__(new_dset[i])\n",
        "  new_dset_norm = torch.cat((new_dset_norm, sample.view(1, 3, 32, 32)))\n",
        "\n",
        "new_dset_norm = torch.tensor(new_dset_norm, requires_grad=True)\n",
        "    batch_num = X.shape[0] // batch_size\n",
        "    for i in range(batch_num):\n",
        "        X_batch = X[i * batch_size:(i + 1) * batch_size, :, :, :]\n",
        "        y_batch = y[i * batch_size:(i + 1) * batch_size]\n",
        "        yield (X_batch, y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sny_Jp5YAXAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "3dd02a95-8d25-43be-9648-566680ca5529"
      },
      "source": [
        "%%time\n",
        "# model = ConvClassyfier().cuda()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "total_step = 100\n",
        "\n",
        "for epoch in range(5):\n",
        "   truck_batch_generator = batch_generator(new_dset_norm, new_dset_labels, batch_size=50)\n",
        "   for i, batch in enumerate(truck_batch_generator):\n",
        "     images = batch[0].to(device)\n",
        "     labels = batch[1].to(device)\n",
        "\n",
        "     outputs = model(images)\n",
        "     loss = criterion(outputs, labels)\n",
        "     loss_list.append(loss.item())\n",
        "\n",
        "     optimizer.zero_grad()\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     if i % 50 == 0:\n",
        "         print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n",
        "             .format(epoch + 1, 5, i, total_step, loss))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [0/100], Loss: 56.02851867675781\n",
            "Epoch [1/5], Step [50/100], Loss: 0.0\n",
            "Epoch [2/5], Step [0/100], Loss: 0.0\n",
            "Epoch [2/5], Step [50/100], Loss: 0.0\n",
            "Epoch [3/5], Step [0/100], Loss: 0.0\n",
            "Epoch [3/5], Step [50/100], Loss: 0.0\n",
            "Epoch [4/5], Step [0/100], Loss: 0.0\n",
            "Epoch [4/5], Step [50/100], Loss: 0.0\n",
            "Epoch [5/5], Step [0/100], Loss: 0.0\n",
            "Epoch [5/5], Step [50/100], Loss: 0.0\n",
            "CPU times: user 4.98 s, sys: 1.92 s, total: 6.9 s\n",
            "Wall time: 6.99 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yajsPCrKg2tK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "73f9d5e8-add2-4c34-f261-5193803a013a"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "for i, batch in enumerate(test_loader):\n",
        "  indices = torch.where(batch[1] == 9)\n",
        "  images, labels = batch[0][indices], batch[1][indices]\n",
        "  images = images.to(device)\n",
        "  labels = labels.to(device)\n",
        "  outputs = model(images)\n",
        "  _, predicted = torch.max(outputs.data, 1)  # Выбор лучшего класса из выходных данных: класс с лучшим счетом\n",
        "  total += labels.size(0)                    # Увеличиваем суммарный счет\n",
        "  correct += (predicted == labels).sum()     # Увеличиваем корректный счет\n",
        "     \n",
        "print('Accuracy of the network on the truck test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the truck test images: 100 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaFZm4uIhNcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "01763475-d975-446b-bbd5-e273472bfa4c"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "for i, batch in enumerate(test_loader):\n",
        "  # indices = torch.where(batch[1] != 9)\n",
        "  images, labels = batch[0], batch[1]\n",
        "  images = images.to(device)\n",
        "  labels = labels.to(device)\n",
        "  outputs = model(images)\n",
        "  _, predicted = torch.max(outputs.data, 1)  # Выбор лучшего класса из выходных данных: класс с лучшим счетом\n",
        "  total += labels.size(0)                    # Увеличиваем суммарный счет\n",
        "  correct += (predicted == labels).sum()     # Увеличиваем корректный счет\n",
        "     \n",
        "print('Accuracy of the network on the truck test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the truck test images: 10 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U46jBkfJyXzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}